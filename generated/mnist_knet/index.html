<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MNIST: Knet.jl CNN · Augmentor.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="Augmentor.jl logo"/></a><h1>Augmentor.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../gettingstarted/">Getting Started</a></li><li><span class="toctext">Introduction and Motivation</span><ul><li><a class="toctext" href="../../background/">Background and Motivation</a></li><li><a class="toctext" href="../../images/">Working with Images in Julia</a></li></ul></li><li><span class="toctext">User&#39;s Guide</span><ul><li><a class="toctext" href="../../interface/">High-level Interface</a></li><li><a class="toctext" href="../../operations/">Supported Operations</a></li></ul></li><li><span class="toctext">Tutorials</span><ul><li><a class="toctext" href="../mnist_elastic/">MNIST: Elastic Distortions</a></li><li class="current"><a class="toctext" href>MNIST: Knet.jl CNN</a><ul class="internal"><li><a class="toctext" href="#Preparing-the-MNIST-dataset-1">Preparing the MNIST dataset</a></li><li><a class="toctext" href="#Defining-the-Network-1">Defining the Network</a></li><li><a class="toctext" href="#Training-without-Augmentation-1">Training without Augmentation</a></li><li><a class="toctext" href="#Integrating-Augmentor-1">Integrating Augmentor</a></li><li><a class="toctext" href="#Visualizing-the-Results-1">Visualizing the Results</a></li><li><a class="toctext" href="#References-1">References</a></li></ul></li></ul></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li>Tutorials</li><li><a href>MNIST: Knet.jl CNN</a></li></ul><a class="edit-page" href="https://github.com/Evizero/Augmentor.jl/blob/master/examples/mnist_knet.jl"><span class="fa"></span> Edit on GitHub</a><a class="edit-page" href="https://nbviewer.jupyter.org/github/Evizero/Augmentor.jl/blob/gh-pages/generated/mnist_knet.ipynb"><span class="fa fa-external-link"> </span> Juypter Notebook</a></nav><hr/><div id="topbar"><span>MNIST: Knet.jl CNN</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="MNIST:-Knet.jl-CNN-1" href="#MNIST:-Knet.jl-CNN-1">MNIST: Knet.jl CNN</a></h1><p>In this tutorial we will adapt the <a href="http://denizyuret.github.io/Knet.jl/latest/tutorial.html#Convolutional-neural-network-1">MNIST example</a> from <a href="https://github.com/denizyuret/Knet.jl">Knet.jl</a> to utilize a custom augmentation pipeline. In order to showcase the effect that image augmentation can have on a neural network&#39;s ability to generalize, we will limit the training set to just the first 500 images (of the available 60,000!). For more information on the dataset see <a href="#footnote-MNIST1998">[MNIST1998]</a>.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>This tutorial is also available as a <a href="https://jupyter.org/">Juypter</a> notebook. You can find a link to the Juypter version of this tutorial in the top right corner of this page.</p></div></div><h2><a class="nav-anchor" id="Preparing-the-MNIST-dataset-1" href="#Preparing-the-MNIST-dataset-1">Preparing the MNIST dataset</a></h2><p>In order to access, prepare, and visualize the MNIST images we employ the help of three additional Julia packages. In the interest of time and space we will not go into great detail about their functionality. Feel free to click on their respective names to find out more information about the utility they can provide.</p><ul><li><p><a href="https://github.com/JuliaML/MLDatasets.jl">MLDatasets.jl</a> has an MNIST submodule that offers a convenience interface to read the MNIST database.</p></li><li><p><a href="https://github.com/JuliaImages/Images.jl">Images.jl</a> will provide us with the necessary tools to process and display the image data in Julia / Juypter.</p></li><li><p><a href="https://github.com/JuliaML/MLDataUtils.jl">MLDataUtils.jl</a> implements a variety of functions to convert and partition Machine Learning datasets. This will help us prepare the MNIST data to be used with Knet.jl.</p></li></ul><div><pre><code class="language-julia">using Images, MLDatasets, MLDataUtils
srand(42);</code></pre></div><p>As you may have seen previously in the <a href="../mnist_elastic/#elastic-1">elastic distortions tutorial</a>, the function <code>MNIST.traintensor</code> returns the MNIST training images corresponding to the given indices as a multi-dimensional array. These images are stored in the native horizontal-major memory layout as a single array. Because we specify that the <code>eltype</code> of that array should be <code>Float32</code>, all the individual values are scaled to be between <code>0.0</code> and <code>1.0</code>. Also note, how the observations are laid out along the last array dimension</p><div><pre><code class="language-julia">@show summary(MNIST.traintensor(Float32, 1:500));</code></pre><pre><code class="language-none">summary(MNIST.traintensor(Float32, 1:500)) = &quot;28×28×500 Array{Float32,3}&quot;</code></pre></div><p>The corresponding label of each image is stored as an integer value between <code>0</code> and <code>9</code>. That means that if the label has the value <code>3</code>, then the corresponding image is known to be a handwritten &quot;3&quot;. To show a more concrete example, the following code reveals that the first training image denotes a &quot;5&quot; and the second training image a &quot;0&quot; (etc).</p><div><pre><code class="language-julia">@show summary(MNIST.trainlabels(1:500))
println(&quot;First eight labels: &quot;, join(MNIST.trainlabels(1:8),&quot;, &quot;))</code></pre><pre><code class="language-none">summary(MNIST.trainlabels(1:500)) = &quot;500-element Array{Int64,1}&quot;
First eight labels: 5, 0, 4, 1, 9, 2, 1, 3</code></pre></div><p>For Knet we will require a slightly format for the images and also the labels. More specifically, we add an additional singleton dimension of length 1 to our image array. Think of this as our single color channel (because MNIST images are gray). Additionally we will convert our labels to proper 1-based indices. This is because some functions provided by Knet expect the labels to be in this format. We will do all this by creating a little utility function that we will name <code>prepare_mnist</code>.</p><div><pre><code class="language-julia">&quot;&quot;&quot;
    prepare_mnist(images, labels) -&gt; (X, Y)

Change the dimension layout x1×x2×N of the given array
`images` to x1×x2×1×N and return the result as `X`.
The given integer vector `labels` is transformed into
an integer vector denoting 1-based class indices.
&quot;&quot;&quot;
function prepare_mnist(images, labels)
    X = reshape(images, (28, 28, 1, :))
    Y = convertlabel(LabelEnc.Indices{Int8}, labels, 0:9)
    X, Y
end</code></pre></div><p>With <code>prepare_mnist</code> defined, we can now use it in conjunction with the functions in the <code>MLDatasets.MNIST</code> sub-module to load and prepare our training set. Recall that for this tutorial only the first 500 images of the training set will be used.</p><div><pre><code class="language-julia">train_x, train_y = prepare_mnist(MNIST.traintensor(Float32, 1:500), MNIST.trainlabels(1:500))
@show summary(train_x) summary(train_y);
[MNIST.convert2image(train_x[:,:,1,i]) for i in 1:8]</code></pre><pre><code class="language-none">summary(train_x) = &quot;28×28×1×500 Array{Float32,4}&quot;
summary(train_y) = &quot;500-element Array{Int8,1}&quot;</code></pre></div><p><img src="../mnist_knet_train.png" alt="training images"/></p><p>Similarly, we use <code>MNIST.testtensor</code> and <code>MNIST.testlabels</code> to load the full MNIST test set. We will utilize that data to measure how well the network is able to generalize with and without augmentation.</p><div><pre><code class="language-julia">test_x, test_y = prepare_mnist(MNIST.testtensor(Float32), MNIST.testlabels())
@show summary(test_x) summary(test_y);
[MNIST.convert2image(test_x[:,:,1,i]) for i in 1:8]</code></pre><pre><code class="language-none">summary(test_x) = &quot;28×28×1×10000 Array{Float32,4}&quot;
summary(test_y) = &quot;10000-element Array{Int8,1}&quot;</code></pre></div><p><img src="../mnist_knet_test.png" alt="test images"/></p><h2><a class="nav-anchor" id="Defining-the-Network-1" href="#Defining-the-Network-1">Defining the Network</a></h2><p>With the dataset prepared, we can now define and instantiate our neural network. To keep things simple, we will use the same convolutional network as defined in the <a href="http://denizyuret.github.io/Knet.jl/latest/tutorial.html#Convolutional-neural-network-1">MNIST example</a> of the Knet.jl package.</p><div><pre><code class="language-julia">using Knet</code></pre></div><p>The first thing we will do is define the forward pass through the network. This will effectively outline the computation graph of the network architecture. Note how this does not define some details, such as the number of neurons per layer. We will define those later when initializing our vector of weight arrays <code>w</code>.</p><div><pre><code class="language-julia">&quot;&quot;&quot;
    forward(w, x) -&gt; a

Compute the forward pass for the given minibatch `x` by using the
neural network parameters in `w`. The resulting (unnormalized)
activations of the last layer are returned as `a`.
&quot;&quot;&quot;
function forward(w, x)
    # conv1 (2x2 maxpool)
    a1 = pool(relu.(conv4(w[1], x)  .+ w[2]))
    # conv2 (2x2 maxpool)
    a2 = pool(relu.(conv4(w[3], a1) .+ w[4]))
    # dense1 (relu)
    a3 = relu.(w[5] * mat(a2) .+ w[6])
    # dense2 (identity)
    a4 = w[7] * a3 .+ w[8]
    return a4
end</code></pre></div><p>In order to be able to train our network we need to choose a cost function. Because this is a classification problem we will use the negative log-likelihood (provided by <code>Knet.nll</code>). With the cost function defined we can the simply use the higher-order function <code>grad</code> to create a new function <code>costgrad</code> that computes us the corresponding gradients.</p><div><pre><code class="language-julia">&quot;&quot;&quot;
    cost(w, x, y) -&gt; AbstractFloat

Compute the per-instance negative log-likelihood for the data
in the minibatch `(x, y)` given the network with the current
parameters in `w`.
&quot;&quot;&quot;
cost(w, x, y) = nll(forward(w, x), y)
costgrad = grad(cost)</code></pre></div><p>Aside from the cost function that we need for training, we would also like a more interpretable performance measurement. In this tutorial we will use &quot;accuracy&quot; for its simplicity and because we know that the class distribution for MNIST is close to uniform.</p><div><pre><code class="language-julia">&quot;&quot;&quot;
    acc(w, X, Y; [batchsize]) -&gt; Float64

Compute the accuracy for the data in `(X,Y)` given the network
with the current parameters in `w`. The resulting value is
computed by iterating over the data in minibatches of size
`batchsize`.
&quot;&quot;&quot;
function acc(w, X, Y; batchsize = 100)
    sum = 0; count = 0
    for (x_cpu, y) in eachbatch((X, Y), maxsize = batchsize)
        x = KnetArray{Float32}(x_cpu)
        sum += Int(accuracy(forward(w,x), y, average = false))
        count += length(y)
    end
    return sum / count
end</code></pre></div><p>Before we can train or even just use our network, we need to define how we initialize <code>w</code>, which is our the vector of parameter arrays. The dimensions of these individual arrays specify the filter sizes and number of neurons. It can be helpful to compare the indices here with the indices used in our <code>forward</code> function to see which array corresponds to which computation node of our network.</p><div><pre><code class="language-julia">function weights(atype = KnetArray{Float32})
    w = Array{Any}(8)
    # conv1
    w[1] = xavier(5,5,1,20)
    w[2] = zeros(1,1,20,1)
    # conv2
    w[3] = xavier(5,5,20,50)
    w[4] = zeros(1,1,50,1)
    # dense1
    w[5] = xavier(500,800)
    w[6] = zeros(500,1)
    # dense2
    w[7] = xavier(10,500)
    w[8] = zeros(10,1)
    return map(a-&gt;convert(atype,a), w)
end</code></pre></div><h2><a class="nav-anchor" id="Training-without-Augmentation-1" href="#Training-without-Augmentation-1">Training without Augmentation</a></h2><p>In order to get an intuition for how useful augmentation can be, we need a sensible baseline to compare to. To that end, we will first train the network we just defined using only the (unaltered) 500 training examples.</p><p>The package <a href="https://github.com/JuliaML/ValueHistories.jl">ValueHistories.jl</a> will help us record the accuracy during the training process. We will use those logs later to visualize the differences between having augmentation or no augmentation.</p><div><pre><code class="language-julia">using ValueHistories</code></pre></div><p>To keep things simple, we will not overly optimize our training function. Thus, we will be content with using a closure. Because both, the baseline and the augmented version, will share this &quot;inefficiency&quot;, we should still get a decent enough picture of their performance differences.</p><div><pre><code class="language-julia">function train_baseline(; epochs = 500, batchsize = 100, lr = .03)
    w = weights()
    log = MVHistory()
    for epoch in 1:epochs
        for (batch_x_cpu, batch_y) in eachbatch((train_x ,train_y), batchsize)
            batch_x = KnetArray{Float32}(batch_x_cpu)
            g = costgrad(w, batch_x, batch_y)
            Knet.update!(w, g, lr = lr)
        end

        if (epoch % 5) == 0
            train = acc(w, train_x, train_y)
            test  = acc(w, test_x,  test_y)
            @trace log epoch train test
            msg = &quot;epoch &quot; * lpad(epoch,4) * &quot;: train accuracy &quot; * rpad(round(train,3),5,&quot;0&quot;) * &quot;, test accuracy &quot; * rpad(round(test,3),5,&quot;0&quot;)
            println(msg)
        end
    end
    log
end</code></pre></div><p>Aside from the accuracy, we will also keep an eye on the training time. In particular we would like to see if and how the addition of augmentation causes our training time to increase.</p><div><pre><code class="language-julia">train_baseline(epochs=1) # warm-up
baseline_log = @time train_baseline(epochs=200);</code></pre><pre><code class="language-none">epoch    5: train accuracy 0.602, test accuracy 0.504
epoch   10: train accuracy 0.746, test accuracy 0.661
epoch   15: train accuracy 0.814, test accuracy 0.750
epoch   20: train accuracy 0.848, test accuracy 0.790
epoch   25: train accuracy 0.896, test accuracy 0.831
epoch   30: train accuracy 0.902, test accuracy 0.858
epoch   35: train accuracy 0.922, test accuracy 0.866
epoch   40: train accuracy 0.932, test accuracy 0.872
epoch   45: train accuracy 0.940, test accuracy 0.877
epoch   50: train accuracy 0.956, test accuracy 0.880
epoch   55: train accuracy 0.966, test accuracy 0.884
epoch   60: train accuracy 0.970, test accuracy 0.886
epoch   65: train accuracy 0.978, test accuracy 0.888
epoch   70: train accuracy 0.988, test accuracy 0.888
epoch   75: train accuracy 0.990, test accuracy 0.890
epoch   80: train accuracy 0.992, test accuracy 0.892
epoch   85: train accuracy 0.996, test accuracy 0.893
epoch   90: train accuracy 1.000, test accuracy 0.894
epoch   95: train accuracy 1.000, test accuracy 0.894
epoch  100: train accuracy 1.000, test accuracy 0.895
epoch  105: train accuracy 1.000, test accuracy 0.895
epoch  110: train accuracy 1.000, test accuracy 0.895
epoch  115: train accuracy 1.000, test accuracy 0.895
epoch  120: train accuracy 1.000, test accuracy 0.896
epoch  125: train accuracy 1.000, test accuracy 0.896
epoch  130: train accuracy 1.000, test accuracy 0.897
epoch  135: train accuracy 1.000, test accuracy 0.896
epoch  140: train accuracy 1.000, test accuracy 0.897
epoch  145: train accuracy 1.000, test accuracy 0.897
epoch  150: train accuracy 1.000, test accuracy 0.897
epoch  155: train accuracy 1.000, test accuracy 0.897
epoch  160: train accuracy 1.000, test accuracy 0.897
epoch  165: train accuracy 1.000, test accuracy 0.897
epoch  170: train accuracy 1.000, test accuracy 0.897
epoch  175: train accuracy 1.000, test accuracy 0.897
epoch  180: train accuracy 1.000, test accuracy 0.897
epoch  185: train accuracy 1.000, test accuracy 0.897
epoch  190: train accuracy 1.000, test accuracy 0.897
epoch  195: train accuracy 1.000, test accuracy 0.897
epoch  200: train accuracy 1.000, test accuracy 0.898
 12.875449 seconds (3.16 M allocations: 284.518 MiB, 1.74% gc time)</code></pre></div><p>As we can see, the accuracy on the training set is around a 100%, while the accuracy on the test set peaks around 90%. For a mere 500 training examples, this isn&#39;t actually that bad of a result.</p><h2><a class="nav-anchor" id="Integrating-Augmentor-1" href="#Integrating-Augmentor-1">Integrating Augmentor</a></h2><p>Now that we have a network architecture with a baseline to compare to, let us finally see what it takes to add Augmentor to our experiment. First, we need to include the package to our experiment.</p><div><pre><code class="language-julia">using Augmentor</code></pre></div><p>The next step, and maybe the most human-hour consuming part of adding image augmentation to a prediction problem, is to design and select a sensible augmentation pipeline. Take a look at the <a href="../mnist_elastic/#elastic-1">elastic distortions tutorial</a> for an example of how to do just that.</p><p>For this example, we already choose a quite complicated but promising augmentation pipeline for you. This pipeline was designed to yield a large variation of effects as well as to showcase how even deep pipelines are quite efficient in terms of performance.</p><div><pre><code class="language-julia">pl = Reshape(28,28) |&gt;
     PermuteDims(2,1) |&gt;
     ShearX(-5:5) * ShearY(-5:5) |&gt;
     Rotate(-15:15) |&gt;
     CropSize(28,28) |&gt;
     Zoom(0.9:0.1:1.2) |&gt;
     CacheImage() |&gt;
     ElasticDistortion(10) |&gt;
     PermuteDims(2,1) |&gt;
     Reshape(28,28,1)</code></pre><pre><code class="language-none">10-step Augmentor.ImmutablePipeline:
  1.) Reshape array to 28×28
  2.) Permute dimension order to (2, 1)
  3.) Either: (50%) ShearX by ϕ ∈ -5:5 degree. (50%) ShearY by ψ ∈ -5:5 degree.
  4.) Rotate by θ ∈ -15:15 degree
  5.) Crop a 28×28 window around the center
  6.) Zoom by I ∈ {0.9×0.9, 1.0×1.0, 1.1×1.1, 1.2×1.2}
  7.) Cache into temporary buffer
  8.) Distort using a smoothed and normalized 10×10 grid with pinned border
  9.) Permute dimension order to (2, 1)
 10.) Reshape array to 28×28×1</code></pre></div><p>Most of the used operations are quite self explanatory, but there are some details about this pipeline worth pointing out explicitly.</p><ol><li><p>We use the operation <a href="../../operations/permutedims/#Augmentor.PermuteDims"><code>PermuteDims</code></a> to convert the horizontal-major MNIST image to a julia-native vertical-major image. The vertical-major image is then processed and converted back to a horizontal-major array. We mainly do this here to showcase the option, but it is also to keep consistent with how the data is usually used in the literature. Alternatively, one could just work with the MNIST data in a vertical-major format all the way through without any issue.</p></li></ol><ol><li><p>As counter-intuitive as it sounds, the operation <a href="../../operations/cacheimage/#Augmentor.CacheImage"><code>CacheImage</code></a> right before <a href="../../operations/elasticdistortion/#Augmentor.ElasticDistortion"><code>ElasticDistortion</code></a> is actually used to improve performance. If we were to omit it, then the whole pipeline would be applied in one single pass. In this case, applying distortions on top of affine transformations lazily is in fact less efficient than using a temporary variable.</p></li></ol><p>With the pipeline now defined, let us quickly peek at what kind of effects we can achieve with it. In particular, lets apply the pipeline multiple times to the first training image and look at what kind of results it produces.</p><div><pre><code class="language-julia">[MNIST.convert2image(reshape(augment(train_x[:,:,:,1], pl), (28, 28))) for i in 1:8, j in 1:2]</code></pre></div><p><img src="../mnist_knet_aug.png" alt="augmented samples"/></p><p>As we can see, we can achieve a wide range of effects, from more subtle to more pronounced. The important part is that all examples are still clearly representative of the true label.</p><p>Next, we have to adapt the function <code>train_baseline</code> to make use of our augmentation pipeline. To integrate Augmentor efficiently, there are three necessary changes we have to make.</p><ol><li><p>Preallocate a buffer with the same size and element type that each batch has.</p><pre><code class="language-none">batch_x_aug = zeros(Float32, 28, 28, 1, batchsize)</code></pre></li></ol><ol><li><p>Add a call to <a href="../../interface/#Augmentor.augmentbatch!"><code>augmentbatch!</code></a> in the inner loop of the batch iterator using our pipeline and buffer.</p><pre><code class="language-none">augmentbatch!(batch_x_aug, batch_x_org, pl)</code></pre></li></ol><ol><li><p>Replace <code>batch_x_org</code> with <code>batch_x_aug</code> in the constructor of <code>KnetArray</code>.</p><pre><code class="language-none">batch_x = KnetArray{Float32}(batch_x_aug)</code></pre></li></ol><p>Applying these changes to our <code>train_baseline</code> function will give us something similar to the following function. Note how all the other parts of the function remain exactly the same as before.</p><div><pre><code class="language-julia">function train_augmented(; epochs = 500, batchsize = 100, lr = .03)
    w = weights()
    log = MVHistory()
    batch_x_aug = zeros(Float32, size(train_x,1), size(train_x,2), 1, batchsize)
    for epoch in 1:epochs
        for (batch_x_cpu, batch_y) in eachbatch((train_x ,train_y), batchsize)
            augmentbatch!(CPUThreads(), batch_x_aug, batch_x_cpu, pl)
            batch_x = KnetArray{Float32}(batch_x_aug)
            g = costgrad(w, batch_x, batch_y)
            Knet.update!(w, g, lr = lr)
        end

        if (epoch % 5) == 0
            train = acc(w, train_x, train_y)
            test  = acc(w, test_x,  test_y)
            @trace log epoch train test
            msg = &quot;epoch &quot; * lpad(epoch,4) * &quot;: train accuracy &quot; * rpad(round(train,3),5,&quot;0&quot;) * &quot;, test accuracy &quot; * rpad(round(test,3),5,&quot;0&quot;)
            println(msg)
        end
    end
    log
end</code></pre></div><p>You may have noticed in the code above that we also pass a <code>CPUThreads()</code> as the first argument to <a href="../../interface/#Augmentor.augmentbatch!"><code>augmentbatch!</code></a>. This instructs Augmentor to process the images of the batch in parallel using multi-threading. For this to work properly you will need to set the environment variable <code>JULIA_NUM_THREADS</code> to the number of threads you wish to use. You can check how many threads are used with the function <code>Threads.nthreads()</code></p><div><pre><code class="language-julia">@show Threads.nthreads();</code></pre><pre><code class="language-none">Threads.nthreads() = 12</code></pre></div><p>Now that all pieces are in place, let us train our network once more. We will use the same parameters except that now instead of the original training images we will be using randomly augmented images. This will cause every epoch to be different.</p><div><pre><code class="language-julia">train_augmented(epochs=1) # warm-up
augmented_log = @time train_augmented(epochs=200);</code></pre><pre><code class="language-none">epoch    5: train accuracy 0.564, test accuracy 0.494
epoch   10: train accuracy 0.622, test accuracy 0.541
epoch   15: train accuracy 0.734, test accuracy 0.658
epoch   20: train accuracy 0.812, test accuracy 0.765
epoch   25: train accuracy 0.866, test accuracy 0.798
epoch   30: train accuracy 0.846, test accuracy 0.797
epoch   35: train accuracy 0.872, test accuracy 0.826
epoch   40: train accuracy 0.890, test accuracy 0.852
epoch   45: train accuracy 0.904, test accuracy 0.854
epoch   50: train accuracy 0.922, test accuracy 0.869
epoch   55: train accuracy 0.932, test accuracy 0.894
epoch   60: train accuracy 0.938, test accuracy 0.894
epoch   65: train accuracy 0.944, test accuracy 0.912
epoch   70: train accuracy 0.946, test accuracy 0.910
epoch   75: train accuracy 0.954, test accuracy 0.918
epoch   80: train accuracy 0.960, test accuracy 0.919
epoch   85: train accuracy 0.964, test accuracy 0.930
epoch   90: train accuracy 0.940, test accuracy 0.893
epoch   95: train accuracy 0.976, test accuracy 0.935
epoch  100: train accuracy 0.978, test accuracy 0.937
epoch  105: train accuracy 0.970, test accuracy 0.919
epoch  110: train accuracy 0.984, test accuracy 0.939
epoch  115: train accuracy 0.986, test accuracy 0.942
epoch  120: train accuracy 0.988, test accuracy 0.937
epoch  125: train accuracy 0.992, test accuracy 0.937
epoch  130: train accuracy 0.994, test accuracy 0.938
epoch  135: train accuracy 0.992, test accuracy 0.940
epoch  140: train accuracy 0.994, test accuracy 0.942
epoch  145: train accuracy 0.988, test accuracy 0.935
epoch  150: train accuracy 0.998, test accuracy 0.948
epoch  155: train accuracy 0.994, test accuracy 0.949
epoch  160: train accuracy 0.998, test accuracy 0.948
epoch  165: train accuracy 0.998, test accuracy 0.950
epoch  170: train accuracy 0.998, test accuracy 0.951
epoch  175: train accuracy 0.998, test accuracy 0.957
epoch  180: train accuracy 0.998, test accuracy 0.954
epoch  185: train accuracy 0.998, test accuracy 0.952
epoch  190: train accuracy 0.998, test accuracy 0.956
epoch  195: train accuracy 0.998, test accuracy 0.949
epoch  200: train accuracy 0.998, test accuracy 0.954
 31.108804 seconds (37.87 M allocations: 24.084 GiB, 15.05% gc time)</code></pre></div><p>As we can see, our network reaches far better results on our testset than our baseline network did. However, we can also see that the training took quite a bit longer than before. This difference generally decreases as the complexity of the utilized neural network increases. Yet another way to improve performance (aside from simplifying the augmentation pipeline) would be to increase the number of available threads.</p><h2><a class="nav-anchor" id="Visualizing-the-Results-1" href="#Visualizing-the-Results-1">Visualizing the Results</a></h2><p>Before we end this tutorial, let us make use the <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a> package to visualize and discuss the recorded training curves. We will plot the accuracy curves of both networks side by side in order to get a good feeling about their differences.</p><div><pre><code class="language-julia">using Plots
pyplot()</code></pre></div><div><pre><code class="language-julia">plt = plot(
    plot(baseline_log,  title=&quot;Baseline&quot;,  ylim=(.5,1)),
    plot(augmented_log, title=&quot;Augmented&quot;, ylim=(.5,1)),
    size = (900, 400),
    xlab = &quot;Epoch&quot;,
    ylab = &quot;Accuracy&quot;,
    markersize = 1
)</code></pre></div><p><img src="../mnist_knet_curves.png" alt="learning curves"/></p><p>Note how the accuracy on the (unaltered) training set increases faster for the baseline network than for the augmented one. This is to be expected, since our augmented network doesn&#39;t actually use the unaltered images for training, and thus has not actually seen them. Given this information, it is worth pointing out explicitly how the accuracy on training set is still greater than on the test set for the augmented network as well. This is also not a surprise, given that the augmented images are likely more similar to their original ones than to the test images.</p><p>For the baseline network, the accuracy on the test set plateaus quite quickly (around 90%). For the augmented network on the other hand, it the accuracy keeps increasing for quite a while longer.</p><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><div class="footnote" id="footnote-MNIST1998"><a href="#footnote-MNIST1998"><strong>[MNIST1998]</strong></a><p>LeCun, Yan, Corinna Cortes, Christopher J.C. Burges. <a href="http://yann.lecun.com/exdb/mnist/">&quot;The MNIST database of handwritten digits&quot;</a> Website. 1998.</p></div><div></div><footer><hr/><a class="previous" href="../mnist_elastic/"><span class="direction">Previous</span><span class="title">MNIST: Elastic Distortions</span></a><a class="next" href="../../indices/"><span class="direction">Next</span><span class="title">Indices</span></a></footer></article></body></html>

<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MNIST: TensorFlow CNN · Augmentor.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link href="../../assets/style.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="../../index.html"><img class="logo" src="../../assets/logo.png" alt="Augmentor.jl logo"/></a><h1>Augmentor.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../../">Home</a></li><li><a class="toctext" href="../../gettingstarted/">Getting Started</a></li><li><span class="toctext">Introduction and Motivation</span><ul><li><a class="toctext" href="../../background/">Background and Motivation</a></li><li><a class="toctext" href="../../images/">Working with Images in Julia</a></li></ul></li><li><span class="toctext">User&#39;s Guide</span><ul><li><a class="toctext" href="../../interface/">High-level Interface</a></li><li><a class="toctext" href="../../operations/">Supported Operations</a></li></ul></li><li><span class="toctext">Tutorials</span><ul><li><a class="toctext" href="../mnist_elastic/">MNIST: Elastic Distortions</a></li><li class="current"><a class="toctext" href>MNIST: TensorFlow CNN</a><ul class="internal"><li><a class="toctext" href="#Preparing-the-MNIST-dataset-1">Preparing the MNIST dataset</a></li><li><a class="toctext" href="#Defining-the-Network-1">Defining the Network</a></li><li><a class="toctext" href="#Training-without-Augmentation-1">Training without Augmentation</a></li><li><a class="toctext" href="#Integrating-Augmentor-1">Integrating Augmentor</a></li><li><a class="toctext" href="#Visualizing-the-Results-1">Visualizing the Results</a></li><li><a class="toctext" href="#References-1">References</a></li></ul></li></ul></li><li><a class="toctext" href="../../LICENSE/">LICENSE</a></li></ul></nav><article id="docs"><header><nav><ul><li>Tutorials</li><li><a href>MNIST: TensorFlow CNN</a></li></ul><a class="edit-page" href="https://github.com/Evizero/Augmentor.jl/tree/7b9db6cf3ade32c68fe70e4ea5769c13a78296fb/examples/mnist_tensorflow.jl"><span class="fa"></span> Edit on GitHub</a><a class="edit-page" href="https://nbviewer.jupyter.org/github/Evizero/Augmentor.jl/blob/gh-pages/generated/mnist_tensorflow.ipynb"><span class="fa fa-external-link"> </span> Juypter Notebook</a></nav><hr/><div id="topbar"><span>MNIST: TensorFlow CNN</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="MNIST:-TensorFlow-CNN-1" href="#MNIST:-TensorFlow-CNN-1">MNIST: TensorFlow CNN</a></h1><p>In this tutorial we will adapt the <a href="https://github.com/malmaud/TensorFlow.jl/blob/master/examples/mnist_full.jl">MNIST example</a> from <a href="https://github.com/malmaud/TensorFlow.jl">TensorFlow.jl</a> to utilize a custom augmentation pipeline. In order to showcase the effect that image augmentation can have on a neural network&#39;s ability to generalize, we will limit the training set to just the first 500 images (of the available 60,000!). For more information on the dataset see <a href="#footnote-MNIST1998">[MNIST1998]</a>.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>This tutorial is also available as a <a href="https://jupyter.org/">Juypter</a> notebook. You can find a link to the Juypter version of this tutorial in the top right corner of this page.</p></div></div><h2><a class="nav-anchor" id="Preparing-the-MNIST-dataset-1" href="#Preparing-the-MNIST-dataset-1">Preparing the MNIST dataset</a></h2><p>In order to access, prepare, and visualize the MNIST images we employ the help of three additional Julia packages. In the interest of time and space we will not go into great detail about their functionality. Feel free to click on their respective names to find out more information about the utility they can provide.</p><ul><li><p><a href="https://github.com/JuliaML/MLDatasets.jl">MLDatasets.jl</a> has an MNIST submodule that offers a convenience interface to read the MNIST database.</p></li><li><p><a href="https://github.com/JuliaImages/Images.jl">Images.jl</a> will provide us with the necessary tools to process and display the image data in Julia / Juypter.</p></li><li><p><a href="https://github.com/JuliaML/MLDataUtils.jl">MLDataUtils.jl</a> implements a variety of functions to convert and partition Machine Learning datasets. This will help us prepare the MNIST data to be used with TensorFlow.</p></li></ul><div><pre><code class="language-julia">using Images, MLDatasets, MLDataUtils
srand(42);</code></pre></div><p>As you may have seen previously in the <a href="../mnist_elastic/#elastic-1">elastic distortions tutorial</a>, the function <code>MNIST.traintensor</code> returns the MNIST training images corresponding to the given indices as a multi-dimensional array. These images are stored in the native horizontal-major memory layout as a single array of <code>Float64</code>. All the individual values are scaled to be between <code>0.0</code> and <code>1.0</code>. Also note, how the observations are laid out along the last array dimension</p><div><pre><code class="language-julia">@show summary(MNIST.traintensor(1:500));</code></pre><pre><code class="language-none">summary(MNIST.traintensor(1:500)) = &quot;28×28×500 Array{Float64,3}&quot;</code></pre></div><p>The corresponding label of each image is stored as an integer value between <code>0</code> and <code>9</code>. That means that if the label has the value <code>3</code>, then the corresponding image is known to be a handwritten &quot;3&quot;. To show a more concrete example, the following code reveals that the first training image denotes a &quot;5&quot; and the second training image a &quot;0&quot; (etc).</p><div><pre><code class="language-julia">@show summary(MNIST.trainlabels(1:500))
println(&quot;First eight labels: &quot;, join(MNIST.trainlabels(1:8),&quot;, &quot;))</code></pre><pre><code class="language-none">summary(MNIST.trainlabels(1:500)) = &quot;500-element Array{Int64,1}&quot;
First eight labels: 5, 0, 4, 1, 9, 2, 1, 3</code></pre></div><p>For TensorFlow we will require a slightly different dimension layout for the images. More specifically, we will move the observations into the first array dimension. The labels will be transformed into a one-of-k matrix. For performance reasons, we will further convert all the numerical values to be of type <code>Float32</code>. We will do all this by creating a little utility function that we will name <code>prepare_mnist</code>.</p><div><pre><code class="language-julia">&quot;&quot;&quot;
    prepare_mnist(tensor, labels) -&gt; (X, Y)

Change the dimension layout x1×x2×N of the given array
`tensor` to N×x1×x2 and store the result in `X`.
The given vector `labels` is transformed into a 10×N
one-hot matrix `Y`. Both, `X` and `Y`, will have the
element type `Float32`.
&quot;&quot;&quot;
function prepare_mnist(tensor, labels)
    features = convert(Array{Float32}, permutedims(tensor, (3,1,2)))
    targets = convertlabel(LabelEnc.OneOfK{Float32}, labels, 0:9, ObsDim.First())
    features, targets
end</code></pre></div><p>With <code>prepare_mnist</code> defined, we can now use it in conjunction with the functions in the <code>MLDatasets.MNIST</code> sub-module to load and prepare our training set. Recall that for this tutorial only use the first 500 images of the training set will be used.</p><div><pre><code class="language-julia">train_x, train_y = prepare_mnist(MNIST.traintensor(1:500), MNIST.trainlabels(1:500))
@show summary(train_x) summary(train_y);
[MNIST.convert2image(train_x[i,:,:]) for i in 1:8]</code></pre><pre><code class="language-none">summary(train_x) = &quot;500×28×28 Array{Float32,3}&quot;
summary(train_y) = &quot;500×10 Array{Float32,2}&quot;</code></pre></div><p><img src="../mnist_tf_train.png" alt="training images"/></p><p>Similarly, we use <code>MNIST.testtensor</code> and <code>MNIST.testlabels</code> to load the full MNIST test set. We will utilize that data to measure how well the network is able to generalize with and without augmentation.</p><div><pre><code class="language-julia">test_x, test_y = prepare_mnist(MNIST.testtensor(), MNIST.testlabels())
@show summary(test_x) summary(test_y);
[MNIST.convert2image(test_x[i,:,:]) for i in 1:8]</code></pre><pre><code class="language-none">summary(test_x) = &quot;10000×28×28 Array{Float32,3}&quot;
summary(test_y) = &quot;10000×10 Array{Float32,2}&quot;</code></pre></div><p><img src="../mnist_tf_test.png" alt="test images"/></p><h2><a class="nav-anchor" id="Defining-the-Network-1" href="#Defining-the-Network-1">Defining the Network</a></h2><p>With the dataset prepared, we can now instantiate our neural network. To keep things simple, we will use the same convolutional network as defined in the <a href="https://github.com/malmaud/TensorFlow.jl/blob/master/examples/mnist_full.jl">MNIST example</a> of Julia&#39;s TensorFlow package.</p><div><pre><code class="language-julia">using TensorFlow, Distributions
session = Session(Graph());</code></pre><pre><code class="language-none">2017-09-29 02:37:38.813712: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-29 02:37:38.813766: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-29 02:37:38.813794: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-09-29 02:37:38.813810: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-09-29 02:37:38.813823: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-09-29 02:37:39.256419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: Quadro M6000 24GB
major: 5 minor: 2 memoryClockRate (GHz) 1.114
pciBusID 0000:02:00.0
Total memory: 23.86GiB
Free memory: 23.48GiB
2017-09-29 02:37:39.256475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-09-29 02:37:39.256494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-09-29 02:37:39.256526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Quadro M6000 24GB, pci bus id: 0000:02:00.0)</code></pre></div><div><pre><code class="language-julia">function weight_variable(shape...)
    initial = map(Float32, rand(Normal(0, .001), shape...))
    return Variable(initial)
end

function bias_variable(shape...)
    initial = fill(Float32(.1), shape...)
    return Variable(initial)
end

function conv2d(x, W)
    nn.conv2d(x, W, [1, 1, 1, 1], &quot;SAME&quot;)
end

function max_pool_2x2(x)
    nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], &quot;SAME&quot;)
end</code></pre></div><div><pre><code class="language-julia">@tf begin
    x = placeholder(Float32)
    y = placeholder(Float32)

    W_conv1 = weight_variable(5, 5, 1, 32)
    b_conv1 = bias_variable(32)

    x_image = reshape(x, [-1, 28, 28, 1])

    h_conv1 = nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)

    W_conv2 = weight_variable(5, 5, 32, 64)
    b_conv2 = bias_variable(64)

    h_conv2 = nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    h_pool2 = max_pool_2x2(h_conv2)

    W_fc1 = weight_variable(7*7*64, 1024)
    b_fc1 = bias_variable(1024)

    h_pool2_flat = reshape(h_pool2, [-1, 7*7*64])
    h_fc1 = nn.relu(h_pool2_flat * W_fc1 + b_fc1)

    keep_prob = placeholder(Float32)
    h_fc1_drop = nn.dropout(h_fc1, keep_prob)

    W_fc2 = weight_variable(1024, 10)
    b_fc2 = bias_variable(10)

    y_conv = nn.softmax(h_fc1_drop * W_fc2 + b_fc2)

    global cross_entropy = reduce_mean(-reduce_sum(y.*log(y_conv+1e-8), axis=[2]))
    global optimizer = train.minimize(train.AdamOptimizer(1e-4), cross_entropy)

    correct_prediction = broadcast(==, indmax(y_conv, 2), indmax(y, 2))
    global accuracy = reduce_mean(cast(correct_prediction, Float32))
end</code></pre></div><h2><a class="nav-anchor" id="Training-without-Augmentation-1" href="#Training-without-Augmentation-1">Training without Augmentation</a></h2><p>In order to get an intuition for how useful augmentation can be, we need a sensible baseline to compare to. To that end, we will first train the network we just defined using only the (unaltered) 500 training examples.</p><p>The package <a href="https://github.com/JuliaML/ValueHistories.jl">ValueHistories.jl</a> will help us record the accuracy during the training process. We will use those logs later to visualize the differences between having augmentation or no augmentation.</p><div><pre><code class="language-julia">using ValueHistories</code></pre></div><p>To keep things simple, we will not overly optimize our training function. Thus, we will be content with using a closure. Because both, the baseline and the augmented version, will share this &quot;inefficiency&quot;, we should still get a decent enough picture of their performance differences.</p><div><pre><code class="language-julia">function train_baseline(; epochs=500, batchsize=100, reset=true)
    reset &amp;&amp; run(session, global_variables_initializer())
    log = MVHistory()
    for epoch in 1:epochs
        for (batch_x, batch_y) in eachbatch(shuffleobs((train_x, train_y), obsdim=1), size=batchsize, obsdim=1)
            run(session, optimizer, Dict(x=&gt;batch_x, y=&gt;batch_y, keep_prob=&gt;0.5))
        end

        if (epoch % 50) == 0
            train = run(session, accuracy, Dict(x=&gt;train_x, y=&gt;train_y, keep_prob=&gt;1.0))
            test  = run(session, accuracy, Dict(x=&gt;test_x,  y=&gt;test_y,  keep_prob=&gt;1.0))
            @trace log epoch train test
            msg = &quot;epoch &quot; * lpad(epoch,4) * &quot;: train accuracy &quot; * rpad(round(train,3),5,&quot;0&quot;) * &quot;, test accuracy &quot; * rpad(round(test,3),5,&quot;0&quot;)
            println(msg)
        end
    end
    log
end</code></pre></div><p>Aside from the accuracy, we will also keep an eye on the training time. In particular we would like to see if and how the addition of augmentation causes our training time to increase.</p><div><pre><code class="language-julia">train_baseline(epochs=1) # warm-up
baseline_log = @time train_baseline(epochs=1000);</code></pre><pre><code class="language-none">epoch   50: train accuracy 0.628, test accuracy 0.541
epoch  100: train accuracy 0.848, test accuracy 0.747
epoch  150: train accuracy 0.876, test accuracy 0.784
epoch  200: train accuracy 0.902, test accuracy 0.803
epoch  250: train accuracy 0.928, test accuracy 0.815
epoch  300: train accuracy 0.954, test accuracy 0.829
epoch  350: train accuracy 0.968, test accuracy 0.834
epoch  400: train accuracy 0.988, test accuracy 0.838
epoch  450: train accuracy 0.992, test accuracy 0.836
epoch  500: train accuracy 1.000, test accuracy 0.840
epoch  550: train accuracy 1.000, test accuracy 0.841
epoch  600: train accuracy 1.000, test accuracy 0.842
epoch  650: train accuracy 1.000, test accuracy 0.843
epoch  700: train accuracy 1.000, test accuracy 0.849
epoch  750: train accuracy 1.000, test accuracy 0.844
epoch  800: train accuracy 1.000, test accuracy 0.847
epoch  850: train accuracy 1.000, test accuracy 0.853
epoch  900: train accuracy 1.000, test accuracy 0.849
epoch  950: train accuracy 1.000, test accuracy 0.845
epoch 1000: train accuracy 1.000, test accuracy 0.841
 61.100217 seconds (3.14 M allocations: 2.586 GiB, 1.25% gc time)</code></pre></div><p>As we can see, the accuracy on the training set is around a 100%, while the accuracy on the test set peaks around 85%. For a mere 500 training examples, this isn&#39;t actually that bad of a result.</p><h2><a class="nav-anchor" id="Integrating-Augmentor-1" href="#Integrating-Augmentor-1">Integrating Augmentor</a></h2><p>Now that we have a network architecture with a baseline to compare to, let us finally see what it takes to add Augmentor to our experiment. First, we need to include the package to our experiment.</p><div><pre><code class="language-julia">using Augmentor</code></pre></div><p>The next step, and maybe the most human-hour consuming part of adding image augmentation to a prediction problem, is to design and select a sensible augmentation pipeline. Take a look at the <a href="../mnist_elastic/#elastic-1">elastic distortions tutorial</a> for an example of how to do just that.</p><p>For this example, we already choose a quite complicated but promising augmentation pipeline for you. This pipeline was designed to yield a large variation of effects as well as to showcase how even deep pipelines are quite efficient in terms of performance.</p><div><pre><code class="language-julia">pl = PermuteDims(2,1) |&gt;
     ShearX(-5:5) * ShearY(-5:5) |&gt;
     Rotate(-15:15) |&gt;
     CropSize(28,28) |&gt;
     Zoom(0.9:0.1:1.2) |&gt;
     CacheImage() |&gt;
     ElasticDistortion(10) |&gt;
     PermuteDims(2,1)</code></pre><pre><code class="language-none">8-step Augmentor.ImmutablePipeline:
 1.) Permute dimension order to (2, 1)
 2.) Either: (50%) ShearX by ϕ ∈ -5:5 degree. (50%) ShearY by ψ ∈ -5:5 degree.
 3.) Rotate by θ ∈ -15:15 degree
 4.) Crop a 28×28 window around the center
 5.) Zoom by I ∈ {0.9×0.9, 1.0×1.0, 1.1×1.1, 1.2×1.2}
 6.) Cache into temporary buffer
 7.) Distort using a smoothed and normalized 10×10 grid with pinned border
 8.) Permute dimension order to (2, 1)</code></pre></div><p>Most of the used operations are quite self explanatory, but there are some details about this pipeline worth pointing out explicitly.</p><ol><li><p>We use the operation <a href="../../operations/permutedims/#Augmentor.PermuteDims"><code>PermuteDims</code></a> to convert the horizontal-major MNIST image to a julia-native vertical-major image. The vertical-major image is then processed and converted back to a horizontal-major array. We mainly do this here to showcase the option, but it is also to keep consistent with how the data is usually used in the literature. Alternatively, one could just work with the MNIST data in a vertical-major format all the way through without any issue.</p></li><li><p>As counter-intuitive as it sounds, the operation <a href="../../operations/cacheimage/#Augmentor.CacheImage"><code>CacheImage</code></a> right before <a href="../../operations/elasticdistortion/#Augmentor.ElasticDistortion"><code>ElasticDistortion</code></a> is actually used to improve performance. If we were to omit it, then the whole pipeline would be applied in one single pass. In this case, applying distortions on top of affine transformations lazily is in fact less efficient than using a temporary variable.</p></li></ol><p>With the pipeline now defined, let us quickly peek at what kind of effects we can achieve with it. In particular, lets apply the pipeline multiple times to the first training image and look at what kind of results it produces.</p><div><pre><code class="language-julia">[MNIST.convert2image(augment(train_x[1,:,:], pl)) for i in 1:8, j in 1:2]</code></pre></div><p><img src="../mnist_tf_aug.png" alt="augmented samples"/></p><p>As we can see, we can achieve a wide range of effects, from more subtle to more pronounced. The important part is that all examples are still clearly representative of the true label.</p><p>Next, we have to adapt the function <code>train_baseline</code> to make use of our augmentation pipeline. To integrate Augmentor efficiently, there are three necessary changes we have to make.</p><ol><li><p>Preallocate a buffer with the same size and element type that each batch has.</p><pre><code class="language-none">augmented_x = zeros(Float32, batchsize, 28, 28)</code></pre></li><li><p>Add a call to <a href="../../interface/#Augmentor.augmentbatch!"><code>augmentbatch!</code></a> in the inner loop of the batch iterator using our pipeline and buffer.</p><pre><code class="language-none">augmentbatch!(augmented_x, batch_x, pl, ObsDim.First())</code></pre></li><li><p>Replace <code>x=&gt;batch_x</code> with <code>x=&gt;augmented_x</code> in the call to TensorFlow&#39;s <code>run(session, ...)</code>.</p></li></ol><p>Applying these changes to our <code>train_baseline</code> function will give us something similar to the following function. Note how all the other parts of the function remain exactly the same as before.</p><div><pre><code class="language-julia">function train_augmented(; epochs=500, batchsize=100, reset=true)
    reset &amp;&amp; run(session, global_variables_initializer())
    log = MVHistory()
    augm_x = zeros(Float32, batchsize, size(train_x,2), size(train_x,3))
    for epoch in 1:epochs
        for (batch_x, batch_y) in eachbatch(shuffleobs((train_x, train_y), obsdim=1), size=batchsize, obsdim=1)
            augmentbatch!(CPUThreads(), augm_x, batch_x, pl, ObsDim.First())
            run(session, optimizer, Dict(x=&gt;augm_x, y=&gt;batch_y, keep_prob=&gt;0.5))
        end

        if (epoch % 50) == 0
            train = run(session, accuracy, Dict(x=&gt;train_x, y=&gt;train_y, keep_prob=&gt;1.0))
            test  = run(session, accuracy, Dict(x=&gt;test_x,  y=&gt;test_y,  keep_prob=&gt;1.0))
            @trace log epoch train test
            msg = &quot;epoch &quot; * lpad(epoch,4) * &quot;: train accuracy &quot; * rpad(round(train,3),5,&quot;0&quot;) * &quot;, test accuracy &quot; * rpad(round(test,3),5,&quot;0&quot;)
            println(msg)
        end
    end
    log
end</code></pre></div><p>You may have noticed in the code above that we also pass a <code>CPUThreads()</code> as the first argument to <a href="../../interface/#Augmentor.augmentbatch!"><code>augmentbatch!</code></a>. This instructs Augmentor to process the images of the batch in parallel using multi-threading. For this to work properly you will need to set the environment variable <code>JULIA_NUM_THREADS</code> to the number of threads you wish to use. You can check how many threads are used with the function <code>Threads.nthreads()</code></p><div><pre><code class="language-julia">@show Threads.nthreads();</code></pre><pre><code class="language-none">Threads.nthreads() = 12</code></pre></div><p>Now that all pieces are in place, let us train our network once more. We will use the same parameters except that now instead of the original training images we will be using randomly augmented images. This will cause every epoch to be different.</p><div><pre><code class="language-julia">train_augmented(epochs=1) # warm-up
augmented_log = @time train_augmented(epochs=1000);</code></pre><pre><code class="language-none">epoch   50: train accuracy 0.656, test accuracy 0.575
epoch  100: train accuracy 0.824, test accuracy 0.723
epoch  150: train accuracy 0.838, test accuracy 0.761
epoch  200: train accuracy 0.878, test accuracy 0.797
epoch  250: train accuracy 0.884, test accuracy 0.805
epoch  300: train accuracy 0.906, test accuracy 0.823
epoch  350: train accuracy 0.922, test accuracy 0.841
epoch  400: train accuracy 0.920, test accuracy 0.838
epoch  450: train accuracy 0.936, test accuracy 0.856
epoch  500: train accuracy 0.944, test accuracy 0.862
epoch  550: train accuracy 0.952, test accuracy 0.873
epoch  600: train accuracy 0.948, test accuracy 0.863
epoch  650: train accuracy 0.966, test accuracy 0.873
epoch  700: train accuracy 0.964, test accuracy 0.875
epoch  750: train accuracy 0.960, test accuracy 0.876
epoch  800: train accuracy 0.978, test accuracy 0.889
epoch  850: train accuracy 0.980, test accuracy 0.900
epoch  900: train accuracy 0.978, test accuracy 0.896
epoch  950: train accuracy 0.982, test accuracy 0.897
epoch 1000: train accuracy 0.988, test accuracy 0.903
123.952951 seconds (121.11 M allocations: 127.392 GiB, 10.26% gc time)</code></pre></div><p>As we can see, our network reaches far better results on our testset than our baseline network did. However, we can also see that the training took quite a bit longer than before. This difference generally decreases as the complexity of the utilized neural network increases. Yet another way to improve performance (aside from simplifying the augmentation pipeline) would be to increase the number of available threads.</p><h2><a class="nav-anchor" id="Visualizing-the-Results-1" href="#Visualizing-the-Results-1">Visualizing the Results</a></h2><p>Before we end this tutorial, let us make use the <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a> package to visualize and discuss the recorded training curves. We will plot the accuracy curves of both networks side by side in order to get a good feeling about their differences.</p><div><pre><code class="language-julia">using Plots
pyplot()</code></pre></div><div><pre><code class="language-julia">plt = plot(
    plot(baseline_log,  title=&quot;Accuracy (baseline)&quot;,  ylim=(.5,1)),
    plot(augmented_log, title=&quot;Accuracy (augmented)&quot;, ylim=(.5,1)),
    size = (900, 400),
    markersize = 1
)</code></pre></div><p><img src="../mnist_tf_curves.png" alt="learning curves"/></p><p>Note how the accuracy on the (unaltered) training set increases much faster for the baseline network than for the augmented one. This is to be expected, since our augmented network doesn&#39;t actually use the unaltered images for training, and thus has not actually seen them. Given this information, it is worth pointing out explicitly how the accuracy on training set is still greater than on the test set for the augmented network as well. This is also not a surprise, given that the augmented images are likely more similar to their original ones than to the test images.</p><p>For the baseline network, the accuracy on the test set plateaus quite quickly (around 85%). For the augmented network on the other hand, it the accuracy keeps increasing for quite a while longer. If you let the network train long enough you can achieve around 97% even before it stops learning.</p><h2><a class="nav-anchor" id="References-1" href="#References-1">References</a></h2><div class="footnote" id="footnote-MNIST1998"><a href="#footnote-MNIST1998"><strong>[MNIST1998]</strong></a><p>LeCun, Yan, Corinna Cortes, Christopher J.C. Burges. <a href="http://yann.lecun.com/exdb/mnist/">&quot;The MNIST database of handwritten digits&quot;</a> Website. 1998.</p></div><footer><hr/><a class="previous" href="../mnist_elastic/"><span class="direction">Previous</span><span class="title">MNIST: Elastic Distortions</span></a><a class="next" href="../../indices/"><span class="direction">Next</span><span class="title">Indices</span></a></footer></article></body></html>
